{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import sys\n",
    "import gzip\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.utils.data\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training text and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of answers: 1000\n",
      "First 10 answers:,  [2, 2, 1, 1, 1, 1, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "with open('classes.train', 'r') as f:\n",
    "    for line in f:\n",
    "        l = line.strip()\n",
    "        answers.append(int(l))\n",
    "    f.close()\n",
    "print('Number of answers: {}'.format(len(answers)))\n",
    "print('First 10 answers:, ', answers[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs: 1000\n",
      "length of doc with > 1000 words:\n",
      "1010\n",
      "1193\n",
      "1420\n",
      "Max doc length is 1420\n",
      "The first doc:  Good :* much smaller than I expected . It slides easily into a coat pocket . * great image quality . Even a novice / lazy photographer like me can get amazing pictures ! * easy to use ( see above ) Bad :* battery dies when connected to the computer . Buy a spare . * software program not intuitive . Also - \" Olympus Master \" ? Creepy . I edit in the camera or using MS Photo Editor or Photoshop . * no viewfinder means sometimes you 're not sure what you 're shooting . It 's more than a mild annoyance , but in balance it 's okay because you 're getting a great camera at a reduced price .\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "with open('docs.train', 'r') as f:\n",
    "    for line in f:\n",
    "        l = line.strip()\n",
    "        docs.append(l)\n",
    "    f.close()\n",
    "print('Number of docs: {}'.format(len(docs)))\n",
    "max_len = -1\n",
    "for doc in docs:\n",
    "    if len(doc.split()) > max_len:\n",
    "        max_len = len(doc.split())\n",
    "        \n",
    "print('length of doc with > 1000 words:')\n",
    "for doc in docs:\n",
    "    if len(doc.split()) > 1000:\n",
    "        print(len(doc.split()))\n",
    "print('Max doc length is {}'.format(max_len))\n",
    "print('The first doc: ', docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build word2idx\n",
    "threshold is > 1; no modification on the original words (e.g. capital words are kept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct words in docs.train: 10526\n",
      "Size of word to index: 10528\n"
     ]
    }
   ],
   "source": [
    "word2idx = {'[PAD]':0,'[UNK]':1}\n",
    "word_count = {}\n",
    "threshold = 0\n",
    "for doc in docs:\n",
    "    words = doc.split()\n",
    "    for word in words:\n",
    "        if word not in word_count:\n",
    "            word_count[word] = 1\n",
    "        else:\n",
    "            word_count[word] += 1\n",
    "for word, count in word_count.items():\n",
    "    if count > threshold:\n",
    "        word2idx[word] = len(word2idx)\n",
    "\n",
    "print('Number of distinct words in docs.train: {}'.format(len(word_count)))\n",
    "print('Size of word to index: {}'.format(len(word2idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class textDataset(torch.utils.data.dataset.Dataset):\n",
    "    def __init__(self, samples, classes, train=True):\n",
    "        self.samples = samples\n",
    "        self.classes = classes\n",
    "        self.train = train\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        min_augs_sample_length = 100\n",
    "        min_cropped_length = 50\n",
    "        sample = self.samples[index]\n",
    "#         if len(sample) > 1000:\n",
    "#             sample = sample[:1000] # use only the first 1000 tokens\n",
    "        sample_tokenized = sample.split()\n",
    "        sample_indexes = []\n",
    "        for token in sample_tokenized:\n",
    "            if token in word2idx:\n",
    "                sample_indexes.append(int(word2idx[token]))\n",
    "            else:\n",
    "                sample_indexes.append(int(word2idx['[PAD]'])) # NAACL 2015\n",
    "        if len(sample_indexes) > min_augs_sample_length and self.train: # random crop for longer samples\n",
    "            \n",
    "            start = random.randint(0,len(sample_indexes)- min_cropped_length - 1) # expect at least 50 tokens between start and end\n",
    "            end = random.randint(start + min_cropped_length, len(sample_indexes) - 1)\n",
    "        \n",
    "            sample_indexes = sample_indexes[start:end]\n",
    "\n",
    "        if len(sample_indexes) < 1000:\n",
    "            sample_indexes.extend([0]* (1000 - len(sample_indexes)))\n",
    "        if len(sample_indexes) > 1000:\n",
    "            sample_indexes = sample_indexes[:1000]\n",
    "            \n",
    "        # class 1 = new class 0; class 2 = new class 1\n",
    "        cls = int(self.classes[index] -1)\n",
    "        \n",
    "        return torch.tensor(sample_indexes), cls\n",
    "dataset = textDataset(docs, answers)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained word embeddings\n",
    "code modified from the pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>\n",
      "Loading vectors takes 48.799 seconds\n",
      "num small 632919\n",
      "num total 3000000\n",
      "Number of words with pretrained embedding: 8823\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# word2idx = {} # a Python hash table mapping word string to integer index\n",
    "# TODO: to be initialized from the training data\n",
    "with gzip.open(\"../vectors.txt.gz\", \"rt\", encoding=\"utf8\") as f:\n",
    "    # First line contains the embedding vocabulary size and dimension\n",
    "    # .strip() removes the trailing newline\n",
    "    first_line = f.readline().strip()\n",
    "    first_line_tokens = first_line.split(' ')\n",
    "    emb_dim = first_line_tokens[1]\n",
    "    emb_dim = int(emb_dim)\n",
    "    # Embedding is to be stored in a PyTorch tensor, size is\n",
    "    # training_text_vocab_size x emb_dim\n",
    "    # initialized randomly between -0.25 to 0.25\n",
    "    embeddings = torch.rand(len(word2idx), emb_dim) * 0.5 - 0.25\n",
    "    # Read the remaining lines\n",
    "    \n",
    "    # get capitalization information\n",
    "    num_small = 0\n",
    "    num_total = 0\n",
    "    num_have_pretrained_emb = 0\n",
    "    for line in f: # traverse line in file descriptor f\n",
    "        # Remove line trailing newline\n",
    "        line = line.strip()\n",
    "        # Split into word string and vector string\n",
    "        # First space-separated column is the word string\n",
    "        first_space_pos = line.find(' ', 1)\n",
    "        word = line[:first_space_pos]\n",
    "        # Check if word is in word2idx, skip if not\n",
    "        num_total += 1\n",
    "        if word[0].capitalize() != word[0]:\n",
    "            # the word is not capitalized\n",
    "            num_small += 1\n",
    "        # trying to find special tokens\n",
    "        if word[0] == '[' or word[0] == '<':\n",
    "            print(word)\n",
    "        if word not in word2idx:\n",
    "            continue\n",
    "        else:\n",
    "            num_have_pretrained_emb += 1\n",
    "        # Word index corresponds to the row in the embedding tensor\n",
    "        idx = word2idx[word]\n",
    "        # The remaining column is the vector string\n",
    "        emb_str = line[first_space_pos+1:].strip()\n",
    "        # Convert all vector strings into a list of floating point\n",
    "        emb = [float(t) for t in emb_str.split(' ')]\n",
    "        # Convert Python list into PyTorch tensor\n",
    "        embeddings[idx] = torch.tensor(emb)\n",
    "# Do not forget to close the file\n",
    "f.close()\n",
    "end = time.time()\n",
    "print('Loading vectors takes {:.3f} seconds'.format(end-start))\n",
    "print('num small {}'.format(num_small))\n",
    "print('num total {}'.format(num_total))\n",
    "print('Number of words with pretrained embedding: {}'.format(num_have_pretrained_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(embeddings.shape)\n",
    "# print(len(word2idx))\n",
    "# print(emb_dim)\n",
    "# print(list(embeddings.shape))\n",
    "# print([len(word2idx), emb_dim])\n",
    "# print(list(embeddings.shape) == [len(word2idx), emb_dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model\n",
    "embedding dim = 300, as stated in pdf,\n",
    "out_channels = 100, tentative,\n",
    "assume max text length is 1000 (will truncate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (embedding): Embedding(10528, 300, scale_grad_by_freq=True)\n",
       "  (fc): Linear(in_features=20000, out_features=2, bias=True)\n",
       "  (drop): Dropout(p=0.5)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_filters = 1000\n",
    "max_seq_length = 1000\n",
    "stride = 1\n",
    "#padding = kernel_size - 1\n",
    "\n",
    "# fix the number of pooling units and dynamically determine the pooling region size on each\n",
    "# data point so that the entire data is covered without overlapping.\n",
    "# NAACL 2015\n",
    "\n",
    "num_maxpool_outputs = 10\n",
    "\n",
    "conv_mod_list = [3,4] # 1000: NAACL 2015\n",
    "\n",
    "class ConvModules(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super(ConvModules, self).__init__()\n",
    "        padding = kernel_size - 1 \n",
    "        self.conv = nn.Conv1d(in_channels=emb_dim, out_channels=number_of_filters, kernel_size=kernel_size, stride=1, padding=padding)\n",
    "        self.bn = nn.BatchNorm1d(num_features=number_of_filters)\n",
    "        # see torch.nn documentation for the calculation, dilation is 1\n",
    "        self.pool_size = int(((max_seq_length+2*padding-kernel_size)/stride+1)/num_maxpool_outputs)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=self.pool_size, stride=self.pool_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(F.relu(x))\n",
    "        x = self.bn(x) # NAACL 2015 bn after pool\n",
    "        x = x.view(-1, number_of_filters*num_maxpool_outputs)\n",
    "        return x\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=len(word2idx), embedding_dim=emb_dim, scale_grad_by_freq=True, _weight=embeddings)\n",
    "        self.conv_mods = [ConvModules(i).to(device) for i in conv_mod_list] # any other ways to put submodules on GPU?\n",
    "        #self.bn = nn.BatchNorm1d(num_features=number_of_filters * len(self.conv_mods))\n",
    "        self.fc = nn.Linear(in_features=number_of_filters * len(self.conv_mods)*num_maxpool_outputs, out_features=2)\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "        # freeze the embedding layer\n",
    "#         for param in self.embedding.parameters():\n",
    "#             param.requires_grad = False\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        # print('emb x', x.shape) need to reshape, see\n",
    "        # https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&mid=2247489032&idx=4&sn=9e759a05ff5b9da255bae4fcf94e2af5&chksm=ebb42edcdcc3a7ca20eea203dae13105936926e1450f6b67f07010267aa840cc9fb455a66ee2&mpshare=1&scene=1&srcid=0313vVi2q20p2k3TqK6WRGjm&key=5064705dbe24d988c44e89bc5aacade833720db5bed7ef054acb62cd87625a58e4346ea63c5517f60db8e53826aae0a9fd520f9c74acbc5e1aec0c98653314da813162beff57b51a6b19cb03b1724563&ascene=1&uin=MTYxMjc0MjA2NA%3D%3D&devicetype=Windows+10&version=62060728&lang=zh_CN&pass_ticket=dYRZzFLSxLIOmdH5Lwk1l3ymeBOxSu%2B%2FSA7iiOPvUZImOklm%2BihH0Pt8NzO4bH7k\n",
    "        x = x.permute(0,2,1)\n",
    "        x = torch.cat([conv_m(x) for conv_m in self.conv_mods], dim=1)\n",
    "        x = self.fc(self.drop(x)) # dropout following NAACL 2015\n",
    "        return x\n",
    "net = Net()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.01, weight_decay = 0.0005)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, running loss: 186.5204\n",
      "lr:  0.01\n",
      "epoch 1, running loss: 50.5820\n",
      "lr:  0.01\n",
      "epoch 2, running loss: 25.1066\n",
      "lr:  0.01\n",
      "epoch 3, running loss: 7.1287\n",
      "lr:  0.01\n",
      "epoch 4, running loss: 9.8152\n",
      "lr:  0.01\n",
      "epoch 5, running loss: 8.7699\n",
      "lr:  0.01\n",
      "epoch 6, running loss: 16.0904\n",
      "lr:  0.01\n",
      "epoch 7, running loss: 20.6023\n",
      "lr:  0.01\n",
      "epoch 8, running loss: 27.2779\n",
      "lr:  0.01\n",
      "epoch 9, running loss: 11.1746\n",
      "lr:  0.01\n",
      "epoch 10, running loss: 5.7833\n",
      "lr:  0.001\n",
      "epoch 11, running loss: 9.3249\n",
      "lr:  0.001\n",
      "epoch 12, running loss: 4.3535\n",
      "lr:  0.001\n",
      "epoch 13, running loss: 6.8687\n",
      "lr:  0.001\n",
      "epoch 14, running loss: 3.5222\n",
      "lr:  0.001\n",
      "epoch 15, running loss: 5.0858\n",
      "lr:  0.001\n",
      "epoch 16, running loss: 2.0556\n",
      "lr:  0.001\n",
      "epoch 17, running loss: 4.9100\n",
      "lr:  0.001\n",
      "epoch 18, running loss: 1.4182\n",
      "lr:  0.001\n",
      "epoch 19, running loss: 3.1585\n",
      "lr:  0.001\n",
      "epoch 20, running loss: 2.4918\n",
      "lr:  0.00010000000000000002\n",
      "epoch 21, running loss: 3.8448\n",
      "lr:  0.00010000000000000002\n",
      "epoch 22, running loss: 1.9604\n",
      "lr:  0.00010000000000000002\n",
      "epoch 23, running loss: 3.4302\n",
      "lr:  0.00010000000000000002\n",
      "epoch 24, running loss: 3.0765\n",
      "lr:  0.00010000000000000002\n",
      "epoch 25, running loss: 4.2037\n",
      "lr:  0.00010000000000000002\n",
      "epoch 26, running loss: 1.7321\n",
      "lr:  0.00010000000000000002\n",
      "epoch 27, running loss: 2.4164\n",
      "lr:  0.00010000000000000002\n",
      "epoch 28, running loss: 3.4152\n",
      "lr:  0.00010000000000000002\n",
      "epoch 29, running loss: 3.7586\n",
      "lr:  0.00010000000000000002\n",
      "epoch 30, running loss: 2.8058\n",
      "lr:  1.0000000000000003e-05\n",
      "epoch 31, running loss: 1.6354\n",
      "lr:  1.0000000000000003e-05\n",
      "epoch 32, running loss: 2.9493\n",
      "lr:  1.0000000000000003e-05\n",
      "epoch 33, running loss: 1.6186\n",
      "lr:  1.0000000000000003e-05\n",
      "epoch 34, running loss: 2.2122\n",
      "lr:  1.0000000000000003e-05\n",
      "Finished Training\n",
      "Training takes 147.07 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "total_epochs = 35\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    scheduler.step()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    print(\"epoch {}, running loss: {:.4f}\".format(epoch, running_loss))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print('lr: ', param_group['lr'])\n",
    "    running_loss = 0.0\n",
    "print('Finished Training')\n",
    "end = time.time()\n",
    "print('Training takes {:.2f} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test docs: 1000\n",
      "length of doc with > 1000 words:\n",
      "1185\n",
      "Max doc length is 1185\n",
      "The first doc:  I have had the same snow problem as others describe in the reviews . I cleaned the heads - tried everything . One thing the unit does well is heat the room . Samsung produced a portable worldwide heater , capable of warming a small sized bedroom in just under an hour . If the VCR portion doesn't work for you , I recommend using it as a heater . Very satisfied with my heater purchase .\n"
     ]
    }
   ],
   "source": [
    "test_docs = []\n",
    "with open('docs.test', 'r') as f:\n",
    "    for line in f:\n",
    "        l = line.strip()\n",
    "        test_docs.append(l)\n",
    "    f.close()\n",
    "\n",
    "print('Number of test docs: {}'.format(len(test_docs)))\n",
    "max_len = -1\n",
    "for doc in test_docs:\n",
    "    if len(doc.split()) > max_len:\n",
    "        max_len = len(doc.split())\n",
    "        \n",
    "print('length of doc with > 1000 words:')\n",
    "for doc in test_docs:\n",
    "    if len(doc.split()) > 1000:\n",
    "        print(len(doc.split()))\n",
    "print('Max doc length is {}'.format(max_len))\n",
    "print('The first doc: ', test_docs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of answers: 1000\n",
      "First 10 answers:,  [1, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "test_answers = []\n",
    "with open('classes.test', 'r') as f:\n",
    "    for line in f:\n",
    "        l = line.strip()\n",
    "        test_answers.append(int(l))\n",
    "    f.close()\n",
    "print('Number of answers: {}'.format(len(test_answers)))\n",
    "print('First 10 answers:, ', test_answers[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = textDataset(test_docs, test_answers, train=False)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "806\n",
      "Accuracy of the network on the 1000 test docs: 80.6000\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "net.eval()\n",
    "wrong_list = []\n",
    "# with some analysis\n",
    "with torch.no_grad():\n",
    "    for index, data in enumerate(test_dataloader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "#         print('labels ', labels)\n",
    "#         print('predicted, ', predicted.data)\n",
    "        id = 0\n",
    "        for l, p in zip(labels, predicted.data):\n",
    "            if l != p:\n",
    "                wrong_list.append(index * 8 + id)\n",
    "                id += 1\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()# -1 because predictions are 0,1, labels are 1,2\n",
    "print(total)\n",
    "print(correct)\n",
    "print('Accuracy of the network on the 1000 test docs: {:.4f}'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88  -ve\n",
      "106  +ve\n"
     ]
    }
   ],
   "source": [
    "# -ve = 1, +ve = 2\n",
    "num_1 = 0\n",
    "num_2 = 0\n",
    "\n",
    "for i in wrong_list:\n",
    "    #print(len(test_docs[i].split()))\n",
    "    if test_answers[i] == 1:\n",
    "        num_1 += 1\n",
    "    else:\n",
    "        num_2 += 1\n",
    "print(num_1, ' -ve')\n",
    "print(num_2, ' +ve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
