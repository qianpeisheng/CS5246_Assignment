{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import sys\n",
    "import gzip\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training text and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of answers: 1000\n",
      "First 10 answers:,  [2, 2, 1, 1, 1, 1, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "with open('classes.train', 'r') as f:\n",
    "    for line in f:\n",
    "        l = line.strip()\n",
    "        answers.append(int(l))\n",
    "    f.close()\n",
    "print('Number of answers: {}'.format(len(answers)))\n",
    "print('First 10 answers:, ', answers[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs: 1000\n",
      "length of doc with > 1000 words:\n",
      "1010\n",
      "1193\n",
      "1420\n",
      "Max doc length is 1420\n",
      "The first doc:  I recently purchased a Samsung HDTVSamsung LN46A550 46-inch 1080p LCD HDTVwhich I also occasionally use as a computer monitor . This keyboard works great when I 'm sitting on the couch , 15 feet or so away from the computer , and using the TV as my main monitor . When I 'm back at my desk using the desktop monitor , I use the standard keyboard that 's hardwired to the computer . This keyboard costs a little more than most the other wireless keyboards I looked at . However , it got good customer ratings for it 's performance from a distance , and has the mouse pad built in . Those two qualities are what finally sold me on it . For anyone interested , I use an AB switch boxManual Switchbox Db15f for Vga Monitors 2x1 Abto switch to / from the TV monitor and my desktop monitor . In my case , I already have two monitors connected to the video card on the computer . The AB switch allows me to easily alternate between one of those and the TV . Other items you might need if you want to do something similar : 2Tripp Lite P502-006 SVGA Monitor Gold Cable w RGB Coax HD15M / M - 6ftperhaps a longer cable depending on how far your TV is from the computer . Make sure the cables are male on both ends as all the monitor inputs for the above ab box , tv and computer are female . 1Cables To Go - 27412 - 12ft 3.5mm Stereo Audio Cable M / M PC-99 ( Beige ) for connecting your PC line out ( sound ) to the TV . Edit 5/24/2008 : After writing all this about using two monitors via one connection on the PC .. my son pointed out that a vga splitter would be simpler and allow the tv and pc monitor to display the same image simultaneously . Cables To Go - 29550 - 2-Port UXGA Video Splitter Extenderseems like it should do the trick , eliminating the AB switch from the equation . The cable that connects this particular splitter to the PC looks like it needs to be a non-standard M / F though ( looking at the picture ) .\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "with open('docs.train', 'r') as f:\n",
    "    for line in f:\n",
    "        l = line.strip()\n",
    "        docs.append(l)\n",
    "    f.close()\n",
    "print('Number of docs: {}'.format(len(docs)))\n",
    "max_len = -1\n",
    "for doc in docs:\n",
    "    if len(doc.split()) > max_len:\n",
    "        max_len = len(doc.split())\n",
    "        \n",
    "print('length of doc with > 1000 words:')\n",
    "for doc in docs:\n",
    "    if len(doc.split()) > 1000:\n",
    "        print(len(doc.split()))\n",
    "print('Max doc length is {}'.format(max_len))\n",
    "print('The first doc: ', docs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build word2idx\n",
    "threhold is > 1; no modification on the original words (e.g. capital words are kept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct words in docs.train: 10526\n",
      "Size of word to index: 4971\n"
     ]
    }
   ],
   "source": [
    "word2idx = {'[UNK]':0}\n",
    "word_count = {}\n",
    "threshold = 1\n",
    "for doc in docs:\n",
    "    words = doc.split()\n",
    "    for word in words:\n",
    "        if word not in word_count:\n",
    "            word_count[word] = 1\n",
    "        else:\n",
    "            word_count[word] += 1\n",
    "for word, count in word_count.items():\n",
    "    if count > threshold:\n",
    "        word2idx[word] = len(word2idx)\n",
    "\n",
    "print('Number of distinct words in docs.train: {}'.format(len(word_count)))\n",
    "print('Size of word to index: {}'.format(len(word2idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class textDataset(torch.utils.data.dataset.Dataset):\n",
    "    def __init__(self, samples, classes):\n",
    "        self.samples = samples\n",
    "        self.classes = classes\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sample = self.samples[index]\n",
    "        if len(sample) > 1000:\n",
    "            sample = sample[:1000] # use only the first 1000 tokens\n",
    "        sample_tokenized = sample.split()\n",
    "        sample_indexes = []\n",
    "        for token in sample_tokenized:\n",
    "            if token in word2idx:\n",
    "                sample_indexes.append(int(word2idx[token]))\n",
    "            else:\n",
    "                sample_indexes.append(int(word2idx['[UNK]']))\n",
    "        \n",
    "        # if the sample is short, append 0\n",
    "        # shoudl this be different from UNK?\n",
    "        if len(sample_indexes) < 1000:\n",
    "            sample_indexes.extend([0]* (1000 - len(sample_indexes)))\n",
    "        \n",
    "        # class 1 = new class 0; class 2 = new class 1\n",
    "        cls = int(self.classes[index] -1)\n",
    "        \n",
    "        return torch.tensor(sample_indexes), cls\n",
    "dataset = textDataset(docs, answers)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained word embeddings\n",
    "code modified from the pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>\n",
      "Loading vectors takes 50.532 seconds\n",
      "num small 632919\n",
      "num total 3000000\n",
      "Number of words with pretrained embedding: 4670\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# word2idx = {} # a Python hash table mapping word string to integer index\n",
    "# TODO: to be initialized from the training data\n",
    "with gzip.open(\"../vectors.txt.gz\", \"rt\", encoding=\"utf8\") as f:\n",
    "    # First line contains the embedding vocabulary size and dimension\n",
    "    # .strip() removes the trailing newline\n",
    "    first_line = f.readline().strip()\n",
    "    first_line_tokens = first_line.split(' ')\n",
    "    emb_dim = first_line_tokens[1]\n",
    "    emb_dim = int(emb_dim)\n",
    "    # Embedding is to be stored in a PyTorch tensor, size is\n",
    "    # training_text_vocab_size x emb_dim\n",
    "    # initialized randomly between -0.25 to 0.25\n",
    "    embeddings = torch.rand(len(word2idx), emb_dim) * 0.5 - 0.25\n",
    "    # Read the remaining lines\n",
    "    \n",
    "    # get capitalization information\n",
    "    num_small = 0\n",
    "    num_total = 0\n",
    "    num_have_pretrained_emb = 0\n",
    "    for line in f: # traverse line in file descriptor f\n",
    "        # Remove line trailing newline\n",
    "        line = line.strip()\n",
    "        # Split into word string and vector string\n",
    "        # First space-separated column is the word string\n",
    "        first_space_pos = line.find(' ', 1)\n",
    "        word = line[:first_space_pos]\n",
    "        # Check if word is in word2idx, skip if not\n",
    "        num_total += 1\n",
    "        if word[0].capitalize() != word[0]:\n",
    "            # the word is not capitalized\n",
    "            num_small += 1\n",
    "        # trying to find special tokens\n",
    "        if word[0] == '[' or word[0] == '<':\n",
    "            print(word)\n",
    "        if word not in word2idx:\n",
    "            continue\n",
    "        else:\n",
    "            num_have_pretrained_emb += 1\n",
    "        # Word index corresponds to the row in the embedding tensor\n",
    "        idx = word2idx[word]\n",
    "        # The remaining column is the vector string\n",
    "        emb_str = line[first_space_pos+1:].strip()\n",
    "        # Convert all vector strings into a list of floating point\n",
    "        emb = [float(t) for t in emb_str.split(' ')]\n",
    "        # Convert Python list into PyTorch tensor\n",
    "        embeddings[idx] = torch.tensor(emb)\n",
    "# Do not forget to close the file\n",
    "f.close()\n",
    "end = time.time()\n",
    "print('Loading vectors takes {:.3f} seconds'.format(end-start))\n",
    "print('num small {}'.format(num_small))\n",
    "print('num total {}'.format(num_total))\n",
    "print('Number of words with pretrained embedding: {}'.format(num_have_pretrained_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4971, 300])\n",
      "4971\n",
      "300\n",
      "[4971, 300]\n",
      "[4971, 300]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(embeddings.shape)\n",
    "print(len(word2idx))\n",
    "print(emb_dim)\n",
    "print(list(embeddings.shape))\n",
    "print([len(word2idx), emb_dim])\n",
    "print(list(embeddings.shape) == [len(word2idx), emb_dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model\n",
    "embedding dim = 300, as stated in pdf,\n",
    "out_channels = 100, tentative,\n",
    "assume max text length is 1000 (will truncate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (embedding): Embedding(4971, 300)\n",
       "  (conv1): Conv1d(300, 100, kernel_size=(2,), stride=(1,), padding=(1,))\n",
       "  (pool): MaxPool1d(kernel_size=999, stride=999, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc): Linear(in_features=100, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=len(word2idx), embedding_dim=emb_dim, _weight=embeddings)\n",
    "        self.conv1 = nn.Conv1d(in_channels=emb_dim, out_channels=100, kernel_size=2, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size = 1000-2+1)\n",
    "        self.fc = nn.Linear(in_features = 100, out_features = 2)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        # print('emb x', x.shape) need to reshape, see\n",
    "        # https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&mid=2247489032&idx=4&sn=9e759a05ff5b9da255bae4fcf94e2af5&chksm=ebb42edcdcc3a7ca20eea203dae13105936926e1450f6b67f07010267aa840cc9fb455a66ee2&mpshare=1&scene=1&srcid=0313vVi2q20p2k3TqK6WRGjm&key=5064705dbe24d988c44e89bc5aacade833720db5bed7ef054acb62cd87625a58e4346ea63c5517f60db8e53826aae0a9fd520f9c74acbc5e1aec0c98653314da813162beff57b51a6b19cb03b1724563&ascene=1&uin=MTYxMjc0MjA2NA%3D%3D&devicetype=Windows+10&version=62060728&lang=zh_CN&pass_ticket=dYRZzFLSxLIOmdH5Lwk1l3ymeBOxSu%2B%2FSA7iiOPvUZImOklm%2BihH0Pt8NzO4bH7k\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool(F.relu(x))\n",
    "        x = F.dropout(input=x)\n",
    "        x = x.view(-1, 100)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "net = Net()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr = 0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173.2241666316986\n",
      "172.62680304050446\n",
      "171.10860818624496\n",
      "166.51930689811707\n",
      "163.99474176764488\n",
      "158.7173831164837\n",
      "152.78773164749146\n",
      "143.11353754997253\n",
      "136.37956902384758\n",
      "128.5713141709566\n",
      "120.45381104946136\n",
      "111.71452312171459\n",
      "108.76357926428318\n",
      "98.17673103511333\n",
      "89.2974879220128\n",
      "86.41171844303608\n",
      "83.49641493707895\n",
      "79.63828664273024\n",
      "73.5645200908184\n",
      "64.28662864118814\n",
      "62.70934849232435\n",
      "59.29467452317476\n",
      "55.69131428003311\n",
      "53.599727258086205\n",
      "51.795485600829124\n",
      "46.6338609829545\n",
      "41.34715363383293\n",
      "44.30530908703804\n",
      "38.99793766438961\n",
      "36.99736427515745\n",
      "41.70802564918995\n",
      "32.163846768438816\n",
      "32.88529112935066\n",
      "27.97194766998291\n",
      "29.1137617751956\n",
      "26.957217887043953\n",
      "26.402758844196796\n",
      "23.967619754374027\n",
      "27.069493129849434\n",
      "22.097992904484272\n",
      "20.538870438933372\n",
      "20.39600621163845\n",
      "18.33538281172514\n",
      "19.67107642441988\n",
      "19.940915182232857\n",
      "17.754491567611694\n",
      "15.653979979455471\n",
      "15.200925543904305\n",
      "16.867719128727913\n",
      "13.601072020828724\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        #print(inputs.shape)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        #print(outputs)\n",
    "        #print(labels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "#         if i% 200 == 0:\n",
    "#             print('[%d, %5d] loss: %.3f' %\n",
    "#                 (epoch + 1, i + 1, running_loss / 200))\n",
    "    print(running_loss)\n",
    "    running_loss = 0.0\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
